import asyncio
import os
import json
from dotenv import load_dotenv
from openai import AsyncOpenAI
from mcp import ClientSession
from mcp.client.sse import sse_client
from mcp.types import CallToolResult

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤ Config
load_dotenv()
API_KEY = os.getenv("LLM_API_KEY")
BASE_URL = os.getenv("LLM_BASE_URL")
MODEL_NAME = os.getenv("LLM_MODEL", "gpt-4o-mini")
SERVER_URL = "http://localhost:8000/sse"

client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

async def run_complete_agent():
    print(f"üîå ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MCP Server: {SERVER_URL}...")
    
    async with sse_client(SERVER_URL) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            
            # ==========================================
            # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ PROMPTS (‡∏à‡∏≥‡∏•‡∏≠‡∏á User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏°‡∏ô‡∏π)
            # ==========================================
            print("\n--- 1. Loading Prompts ---")
            # ‡∏Ç‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤ Server ‡∏°‡∏µ Prompt ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
            prompts = await session.list_prompts()
            target_prompt_name = "debug_assistant"
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Prompt ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡πÉ‡∏™‡πà argument ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Server ‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
            prompt_result = await session.get_prompt(
                target_prompt_name, 
                arguments={"log_type": "error"}
            )
            
            # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Server
            initial_instruction = prompt_result.messages[0].content.text
            print(f"üìù Prompt Selected: {target_prompt_name}")
            print(f"üìú Instruction: {initial_instruction}")

            # ==========================================
            # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ RESOURCES (‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö)
            # ==========================================
            print("\n--- 2. Loading Resources ---")
            # ‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á AI ‡∏´‡∏£‡∏∑‡∏≠ User ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Resource ‡πÄ‡∏≠‡∏á
            # ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÉ‡∏™‡πà Context ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏¢
            resource_uri = "system://logs"
            resource_content = await session.read_resource(resource_uri)
            log_data = resource_content.contents[0].text
            
            print(f"üì¶ Resource Loaded: {resource_uri}")
            print(f"üìÑ Content Preview: {log_data.replace(chr(10), ' | ')}")

            # ==========================================
            # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° TOOLS ‡πÅ‡∏•‡∏∞ CONTEXT ‡πÉ‡∏´‡πâ AI
            # ==========================================
            print("\n--- 3. AI Processing ---")
            
            # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Tools
            mcp_tools = await session.list_tools()
            openai_tools = [{
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description,
                    "parameters": t.inputSchema
                }
            } for t in mcp_tools.tools]

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡∏¢ (‡∏£‡∏ß‡∏° Prompt + Resource + Instruction)
            messages = [
                {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç (System Admin)"},
                # ‡πÉ‡∏™‡πà Instruction ‡∏à‡∏≤‡∏Å Prompt
                {"role": "user", "content": initial_instruction}, 
                # ‡πÅ‡∏ô‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Resource ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
                {"role": "user", "content": f"‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Log ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å {resource_uri}:\n\n{log_data}"}
            ]

            # ==========================================
            # 4. ‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI (Think -> Act)
            # ==========================================
            # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ OpenAI
            response = await client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                tools=openai_tools,
                tool_choice="auto"
            )
            
            ai_msg = response.choices[0].message
            
            # ‡∏ñ‡πâ‡∏≤ AI ‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ Tool (‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏¢‡∏≤‡∏Å Restart Service)
            if ai_msg.tool_calls:
                print(f"ü§î AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠...")
                messages.append(ai_msg) # ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥

                for tool_call in ai_msg.tool_calls:
                    func_name = tool_call.function.name
                    func_args = json.loads(tool_call.function.arguments)
                    
                    print(f"üöÄ Executing Tool: {func_name} with {func_args}")
                    
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Tool ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà Server
                    result: CallToolResult = await session.call_tool(func_name, arguments=func_args)
                    tool_output = result.content[0].text
                    
                    print(f"‚úÖ Tool Output: {tool_output}")
                    
                    # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ AI
                    messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": func_name,
                        "content": tool_output
                    })

                # ‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                final_res = await client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=messages
                )
                print(f"\nüì¢ AI Final Response:\n{final_res.choices[0].message.content}")
            else:
                print(f"\nüì¢ AI Response:\n{ai_msg.content}")

if __name__ == "__main__":
    asyncio.run(run_complete_agent())